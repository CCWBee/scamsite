# ==============================================================================
# ScamAware Jersey - Guardrails Service Environment Configuration
# ==============================================================================
#
# This file contains environment variables for the Guardrails AI/LLM service.
# This Python-based service handles scam analysis using Ollama-hosted models.
# Copy this file to `.env` and fill in your values.
#
# IMPORTANT:
#   - Never commit the actual `.env` file to version control.
#   - Ensure Ollama is running and accessible before starting this service.
#
# Usage:
#   cp .env.example .env
#   # Edit .env with your actual values
#   python -m uvicorn main:app --reload
#
# ==============================================================================

# ==============================================================================
# ENVIRONMENT
# ==============================================================================

# Application environment
# Valid values: development, staging, production
# - development: enables debug mode, auto-reload, verbose logging, CORS disabled
# - staging: production-like but with additional logging for testing
# - production: optimized performance, strict security, minimal logging
# Default: development
ENVIRONMENT=development

# ==============================================================================
# OLLAMA CONFIGURATION
# ==============================================================================

# Ollama API host URL
# The base URL where Ollama is running
# Format: http(s)://hostname:port
# - Local development: http://localhost:11434
# - Docker Compose: http://ollama:11434 (internal Docker network name)
# - Remote server: http://your-ollama-server:11434
# Default: http://ollama:11434
OLLAMA_HOST=http://ollama:11434

# Ollama model to use for scam analysis
# The model identifier as it appears in Ollama
# Format: model-name:tag
# Recommended models for this use case:
# - qwen2.5:7b-instruct-q4_K_M (balanced performance/quality, ~4GB VRAM)
# - llama3.1:8b-instruct-q4_K_M (good alternative, ~5GB VRAM)
# - mistral:7b-instruct-q4_K_M (fast, ~4GB VRAM)
# Note: The q4_K_M quantization offers good quality with reduced memory usage
# Default: qwen2.5:7b-instruct-q4_K_M
OLLAMA_MODEL=qwen2.5:7b-instruct-q4_K_M

# Ollama request timeout in seconds
# How long to wait for model responses before timing out
# Format: positive integer
# Increase for slower hardware or larger models
# Default: 120
OLLAMA_TIMEOUT=120

# ==============================================================================
# RATE LIMITING
# ==============================================================================

# Maximum API requests per minute per client
# Protects against abuse and ensures fair usage
# Format: positive integer
# - Development: 60 (more lenient for testing)
# - Production: 10 (stricter to prevent abuse)
# Default: 10
RATE_LIMIT_PER_MINUTE=10

# Rate limit window in seconds
# The time window for rate limit calculation
# Format: positive integer
# Default: 60
RATE_LIMIT_WINDOW=60

# ==============================================================================
# LOGGING
# ==============================================================================

# Application log level
# Controls the verbosity of log output
# Valid values: DEBUG, INFO, WARNING, ERROR, CRITICAL
# - DEBUG: verbose, includes all details (development only)
# - INFO: general operational messages
# - WARNING: potential issues that don't prevent operation
# - ERROR: errors that affect functionality
# - CRITICAL: severe errors that may cause shutdown
# Default: INFO
LOG_LEVEL=INFO

# Log format
# Valid values: json, text
# - json: structured logging, better for log aggregation services
# - text: human-readable, better for local development
# Default: text
LOG_FORMAT=text

# ==============================================================================
# API SERVER CONFIGURATION
# ==============================================================================

# API server host binding
# The network interface to bind the server to
# Format: IP address or hostname
# - 0.0.0.0: accept connections from any interface (Docker/production)
# - 127.0.0.1: localhost only (local development)
# Default: 0.0.0.0
API_HOST=0.0.0.0

# API server port
# Format: valid port number (1-65535)
# Default: 8000
API_PORT=8000

# ==============================================================================
# CORS CONFIGURATION
# ==============================================================================

# Allowed CORS origins
# Comma-separated list of allowed origins for cross-origin requests
# Format: comma-separated URLs (no trailing slashes)
# - Development: http://localhost:3000
# - Production: https://scamaware.je
# Use * for development only (not recommended for production)
# Default: http://localhost:3000
CORS_ORIGINS=http://localhost:3000

# ==============================================================================
# SECURITY
# ==============================================================================

# API key for service-to-service authentication (optional)
# If set, requests must include this key in the X-API-Key header
# Format: secure random string (min 32 characters)
# Generate with: openssl rand -hex 32
# Leave empty to disable API key authentication
# Default: empty (disabled)
# API_KEY=your-secure-api-key-here

# Maximum request body size in bytes
# Limits the size of incoming request payloads
# Format: positive integer
# Default: 1048576 (1MB)
MAX_REQUEST_SIZE=1048576

# ==============================================================================
# AI/MODEL SETTINGS
# ==============================================================================

# Maximum tokens for model response
# Limits the length of AI-generated responses
# Format: positive integer
# Default: 1024
MAX_RESPONSE_TOKENS=1024

# Model temperature for response generation
# Controls randomness in AI responses
# Format: float between 0.0 and 2.0
# - 0.0-0.3: more deterministic, consistent responses
# - 0.7-1.0: balanced creativity and consistency
# - 1.0-2.0: more creative, varied responses
# Default: 0.3 (recommended for analysis tasks)
MODEL_TEMPERATURE=0.3

# ==============================================================================
# HEALTH CHECKS
# ==============================================================================

# Enable Ollama connectivity health check
# Valid values: true, false
# When true, health endpoint verifies Ollama is reachable
# Default: true
HEALTH_CHECK_OLLAMA=true

# Health check timeout in seconds
# How long to wait for health check responses
# Format: positive integer
# Default: 5
HEALTH_CHECK_TIMEOUT=5
