# =============================================================================
# ScamAware Jersey - Nginx Development Configuration
# =============================================================================
#
# Purpose:
#   This nginx configuration serves as a reverse proxy for the ScamAware Jersey
#   development environment. It routes requests to the appropriate backend
#   services (Next.js frontend, guardrails API, and Ollama LLM) and provides
#   basic rate limiting protection for the chatbot endpoint.
#
# Architecture Overview:
#   Client Request (Port 80)
#         |
#         v
#   +----------+
#   |  Nginx   |  <-- This configuration
#   +----------+
#         |
#         +---> Frontend (Next.js)     @ localhost:3000
#         |         - Serves the web UI
#         |         - Handles /api/health endpoint
#         |
#         +---> Guardrails Service     @ localhost:8000
#         |         - AI chatbot API with safety guardrails
#         |         - Handles /api/chat endpoint
#         |
#         +---> Ollama                 @ localhost:11434
#                   - Local LLM inference server
#                   - Debug access via /ollama/ prefix
#
# Usage:
#   Development: nginx -c /path/to/nginx.conf
#   Syntax Test: nginx -t -c /path/to/nginx.conf
#
# =============================================================================

# -----------------------------------------------------------------------------
# Worker Processes Configuration
# -----------------------------------------------------------------------------
# 'auto' sets worker processes equal to the number of CPU cores.
# For development, 1 worker is sufficient; 'auto' provides flexibility.
worker_processes auto;

# -----------------------------------------------------------------------------
# Error Logging
# -----------------------------------------------------------------------------
# Log errors to stderr for easy viewing in development.
# Levels: debug, info, notice, warn, error, crit, alert, emerg
error_log stderr info;

# -----------------------------------------------------------------------------
# Events Block
# -----------------------------------------------------------------------------
# Controls how nginx handles connections.
events {
    # Maximum simultaneous connections per worker process.
    # 1024 is more than enough for local development.
    worker_connections 1024;
}

# -----------------------------------------------------------------------------
# HTTP Block - Main HTTP Server Configuration
# -----------------------------------------------------------------------------
http {

    # -------------------------------------------------------------------------
    # MIME Types
    # -------------------------------------------------------------------------
    # Include standard MIME type mappings for serving static files.
    include       mime.types;
    default_type  application/octet-stream;

    # -------------------------------------------------------------------------
    # Logging Format
    # -------------------------------------------------------------------------
    # Custom log format for development - shows useful request details.
    log_format dev_format '$remote_addr - [$time_local] '
                          '"$request" $status $body_bytes_sent '
                          '"$http_referer" "$http_user_agent" '
                          'rt=$request_time uct=$upstream_connect_time';

    # Send access logs to stdout for easy viewing in development.
    access_log /dev/stdout dev_format;

    # -------------------------------------------------------------------------
    # Performance Settings (Development Defaults)
    # -------------------------------------------------------------------------
    # Enable efficient file transfers
    sendfile on;

    # Keep connections alive for 65 seconds
    keepalive_timeout 65;

    # =========================================================================
    # RATE LIMITING CONFIGURATION
    # =========================================================================
    #
    # Rate limiting protects the chatbot API from abuse by limiting how many
    # requests each IP address can make within a time window.
    #
    # How it works:
    #   - Each unique IP ($binary_remote_addr) gets tracked in a shared memory zone
    #   - The zone "chatbot_limit" uses 10MB of memory (enough for ~160,000 IPs)
    #   - Rate of "10r/m" = 10 requests per minute per IP
    #   - Excess requests receive HTTP 429 (Too Many Requests)
    #
    # Why 10r/m for chatbot?
    #   - Prevents spam/abuse of the AI chat endpoint
    #   - Generous enough for normal user interaction
    #   - Protects Ollama from being overwhelmed
    #
    # -------------------------------------------------------------------------
    limit_req_zone $binary_remote_addr zone=chatbot_limit:10m rate=10r/m;

    # Custom error message for rate-limited requests
    limit_req_status 429;

    # =========================================================================
    # UPSTREAM DEFINITIONS
    # =========================================================================
    #
    # Upstreams define the backend servers that nginx will proxy requests to.
    # Using named upstreams makes the configuration cleaner and allows for
    # easy modification of backend addresses.
    #
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # Frontend Upstream - Next.js Development Server
    # -------------------------------------------------------------------------
    # The Next.js application serves:
    #   - All static pages and assets
    #   - Client-side React application
    #   - API routes (except /api/chat which goes to guardrails)
    #   - Hot Module Replacement (HMR) for development
    #
    upstream frontend {
        server localhost:3000;
    }

    # -------------------------------------------------------------------------
    # Guardrails Upstream - AI Safety Layer
    # -------------------------------------------------------------------------
    # The guardrails service handles:
    #   - Chat API requests from the frontend
    #   - Input validation and safety filtering
    #   - Communication with Ollama for LLM inference
    #   - Response sanitization
    #
    upstream guardrails {
        server localhost:8000;
    }

    # -------------------------------------------------------------------------
    # Ollama Upstream - Local LLM Server
    # -------------------------------------------------------------------------
    # Ollama provides:
    #   - Local LLM inference (no external API calls)
    #   - Model management and serving
    #   - Direct API access for debugging purposes
    #
    # NOTE: In production, direct Ollama access should be disabled or
    #       restricted to internal networks only.
    #
    upstream ollama {
        server localhost:11434;
    }

    # =========================================================================
    # SERVER BLOCK - Main Virtual Host
    # =========================================================================
    server {
        # ---------------------------------------------------------------------
        # Listen Directive
        # ---------------------------------------------------------------------
        # Listen on port 80 (HTTP) for all interfaces.
        # For development only - production should use HTTPS (port 443).
        #
        listen 80;

        # Server name - underscore matches any hostname (catch-all)
        server_name _;

        # ---------------------------------------------------------------------
        # Location: /api/chat - Chatbot API (Guardrails Service)
        # ---------------------------------------------------------------------
        # Routes chatbot requests to the guardrails service which:
        #   1. Validates and sanitizes user input
        #   2. Applies safety guardrails
        #   3. Forwards to Ollama for LLM processing
        #   4. Filters and returns the response
        #
        # Rate limiting is applied here to prevent abuse.
        #
        location /api/chat {
            # -----------------------------------------------------------------
            # Rate Limiting
            # -----------------------------------------------------------------
            # Apply the chatbot rate limit zone defined above.
            #
            # burst=5: Allow up to 5 requests to queue during traffic spikes
            #          before returning 429 errors.
            #
            # nodelay: Process burst requests immediately rather than
            #          spacing them out. Better UX for legitimate users.
            #
            limit_req zone=chatbot_limit burst=5 nodelay;

            # -----------------------------------------------------------------
            # Proxy to Guardrails Service
            # -----------------------------------------------------------------
            proxy_pass http://guardrails;

            # -----------------------------------------------------------------
            # Proxy Headers
            # -----------------------------------------------------------------
            # These headers ensure the backend knows the original client info:
            #
            # Host: Original hostname from client request
            proxy_set_header Host $host;

            # X-Real-IP: Client's actual IP address (not nginx's IP)
            proxy_set_header X-Real-IP $remote_addr;

            # X-Forwarded-For: Chain of proxy IPs the request passed through
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

            # X-Forwarded-Proto: Original protocol (http/https)
            proxy_set_header X-Forwarded-Proto $scheme;

            # -----------------------------------------------------------------
            # Timeout Settings
            # -----------------------------------------------------------------
            # AI chat responses can take time, especially with larger models.
            # These generous timeouts prevent premature disconnections.
            #
            # Time to establish connection to upstream
            proxy_connect_timeout 60s;

            # Time to receive response from upstream
            proxy_read_timeout 120s;

            # Time to send request to upstream
            proxy_send_timeout 60s;
        }

        # ---------------------------------------------------------------------
        # Location: /api/health - Health Check Endpoint
        # ---------------------------------------------------------------------
        # Routes health checks to the Next.js frontend.
        # Used by monitoring tools and container orchestrators to verify
        # the application is running correctly.
        #
        location /api/health {
            proxy_pass http://frontend;

            # Standard proxy headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # ---------------------------------------------------------------------
        # Location: /ollama/ - Direct Ollama API Access (Debug Only)
        # ---------------------------------------------------------------------
        # Provides direct access to Ollama's API for debugging purposes.
        # The /ollama/ prefix is stripped before forwarding.
        #
        # Example: /ollama/api/tags -> Ollama receives /api/tags
        #
        # WARNING: This endpoint should be disabled or protected in production!
        #          Direct LLM access bypasses all safety guardrails.
        #
        location /ollama/ {
            # Rewrite to strip the /ollama prefix
            # The trailing slash in proxy_pass handles the prefix stripping
            proxy_pass http://ollama/;

            # Standard proxy headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Timeouts for potentially slow LLM operations
            proxy_connect_timeout 60s;
            proxy_read_timeout 300s;
            proxy_send_timeout 60s;
        }

        # ---------------------------------------------------------------------
        # Location: /_next/webpack-hmr - Next.js Hot Module Replacement
        # ---------------------------------------------------------------------
        # WebSocket endpoint for Next.js development hot reload.
        # When you save a file, Next.js pushes updates through this connection
        # so the browser updates without a full page refresh.
        #
        location /_next/webpack-hmr {
            proxy_pass http://frontend;

            # -----------------------------------------------------------------
            # WebSocket Support
            # -----------------------------------------------------------------
            # WebSocket connections require specific headers to upgrade
            # the HTTP connection to a persistent WebSocket connection.
            #
            # Upgrade: Tells the server to upgrade the connection protocol
            proxy_set_header Upgrade $http_upgrade;

            # Connection: Must be set to "upgrade" for WebSocket
            proxy_set_header Connection "upgrade";

            # Standard proxy headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # -----------------------------------------------------------------
            # WebSocket Timeout
            # -----------------------------------------------------------------
            # Keep the WebSocket connection alive for development.
            # 86400s = 24 hours (prevents timeout during long dev sessions)
            #
            proxy_read_timeout 86400s;
        }

        # ---------------------------------------------------------------------
        # Location: / - Default Route (Next.js Frontend)
        # ---------------------------------------------------------------------
        # Catch-all route for everything else.
        # Sends all unmatched requests to the Next.js frontend.
        #
        location / {
            proxy_pass http://frontend;

            # Standard proxy headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # -----------------------------------------------------------------
            # WebSocket Support for Next.js
            # -----------------------------------------------------------------
            # Next.js may use WebSockets for various features beyond HMR.
            # These headers enable WebSocket upgrades when requested.
            #
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";

            # Reasonable timeouts for page loads
            proxy_connect_timeout 60s;
            proxy_read_timeout 60s;
            proxy_send_timeout 60s;
        }
    }
}
